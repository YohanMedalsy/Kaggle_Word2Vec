{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:56.886920Z",
     "start_time": "2019-10-07T15:41:54.490236Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn2pmml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:56.897924Z",
     "start_time": "2019-10-07T15:41:56.889921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trade Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:56.905926Z",
     "start_time": "2019-10-07T15:41:56.900924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create function with threshold to modify labels\n",
    "def make_regression_labels(threshold, df):\n",
    "    \n",
    "    new_labels = np.zeros_like(df)\n",
    "    new_labels[(df <= -threshold)] = -1\n",
    "    new_labels[(df >=  threshold)] =  1\n",
    "    new_labels[(df ==  0)] = 0\n",
    "\n",
    "#    new_labels = np.sign(df)\n",
    "#    new_labels[(df.abs() <= threshold)] = 0\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T14:51:57.927520Z",
     "start_time": "2019-09-12T14:51:57.910519Z"
    }
   },
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:56.913928Z",
     "start_time": "2019-10-07T15:41:56.908927Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_file_list(mypath = \"C:/Barak/SignalToLocal/\"):\n",
    "    \"\"\"Get File names in a directory. must input the path name.\"\"\"\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    return onlyfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a single file of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:56.931933Z",
     "start_time": "2019-10-07T15:41:56.916928Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_regression(location, filename, latency_thresh):\n",
    "    \"\"\"Load a single file\n",
    "        location = path name\n",
    "        latency_thresh = 37000000\"\"\"\n",
    "    \n",
    "    print(\"Loading \" + filename + \" ...\" )\n",
    "    \n",
    "    #get the date from the filename\n",
    "    fulldate = filename[-12:-8] + '-' + filename[-8:-6] + '-' + filename[-6:-4]\n",
    "    \n",
    "    # The generated files have ---- between the headers and the data\n",
    "    train = pd.read_csv(location + filename, header = 0, skiprows = [1])\n",
    "    train = train.set_index('signalId');\n",
    "    \n",
    "    # Data cleaning\n",
    "    train.dropna(inplace=True)\n",
    "\n",
    "    # number the events within each superCluster\n",
    "    train['superClusterEvent'] = train.groupby(['superClusterId']).cumcount().add(1)\n",
    "\n",
    "    # Calculate how long passed since the beginning of the superCluster\n",
    "    train['timeSinceSCStart'] = (train['sendingTime'] - train.groupby('superClusterId')['sendingTime'].transform('first')) / 1000\n",
    "\n",
    "    train['isReaction'] = train['ltcyAfter'] < latency_thresh\n",
    "\n",
    "    # Renumber the superCluster\n",
    "    train = renumber_scId(train)\n",
    "\n",
    "\n",
    "    #Create target based on the price movement in the Dax.\n",
    "    train['target'] = train.simpleMid_afp - train.simpleMidBefore\n",
    "\n",
    "    #Calculate the latency difference\n",
    "    train['ltcyDiff'] = train['ltcyAfterPlus']-train['ltcyBefore']\n",
    "\n",
    "    #Sell all targets with no reaction to 0\n",
    "    train.loc[(train.isReaction == False),'target'] = 0\n",
    "\n",
    "    #Cast all the sendingtimes to Timestamps\n",
    "    train.sendingTimeFullStr = train.sendingTimeFullStr.apply(lambda x:datetime.strptime(x[0:-1], '%Y-%m-%d %H:%M:%S.%f'))\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load many files in a range of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:56.953940Z",
     "start_time": "2019-10-07T15:41:56.934933Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_range(fromDate = -1, toDate = -1, list_columns=[], save = False, my_path = \"C:/Barak/SignalToLocal/\", reaction_thresh = 37000000):\n",
    "    \"\"\" Load and concatenate a series of files in a date range.\n",
    "        Option to save the concatenated files to the working directory.\n",
    "        list_columns = List of features to add to the default list. Default loads all columns.\n",
    "        fromDate = YYYYMMDD\n",
    "        toDate = YYYYMMDD\n",
    "        Default value for both is all files in the directory.\n",
    "        Default save = False\n",
    "        Default path is C:/Barak/SignalToLocal/\n",
    "        Default reaction_thresh = 37000000\"\"\"\n",
    "    \n",
    "    default_list = ['scId', 'superClusterEvent', 'timeSinceSCStart', 'sendingTimeFullStr', 'target']\n",
    "    my_columns = default_list + list_columns\n",
    "    \n",
    "    concat_data = pd.DataFrame()\n",
    "    first_date = 99999999\n",
    "    last_date = -1\n",
    "    last_scId = 0\n",
    "    for f in listdir(my_path):\n",
    "        if os.stat(my_path + f).st_size > 10000:\n",
    "            date = int(f.replace(\"sigsToLocs.\", \"\").replace(\".csv\", \"\"))\n",
    "            if ((fromDate == -1 & toDate == -1) or (fromDate <= date & date <= toDate)):\n",
    "                new_data = load_data_regression(my_path, f, reaction_thresh)\n",
    "                \n",
    "                new_data['scId'] += last_scId\n",
    "                last_scId = new_data.iloc[-1, :]['scId']\n",
    "                                \n",
    "                if list_columns==[]:\n",
    "                    concat_data = pd.concat([concat_data, new_data])\n",
    "                else:\n",
    "                    concat_data = pd.concat([concat_data, new_data[my_columns]])\n",
    "                    \n",
    "                first_date = min(first_date, date)\n",
    "                last_date = max(last_date, date)\n",
    "\n",
    "    \n",
    "    concat_data = renumber_scId(concat_data)\n",
    "    \n",
    "    #save all files concatenated\n",
    "    if save:\n",
    "        # add dir creation or deprecate function if not used\n",
    "        # no relative paths!!!\n",
    "        concat_data.to_csv('Loaded_Data/loaded_' + str(first_date) + \"_\" + str(last_date) + \".csv\")\n",
    "    \n",
    "    return concat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:56.964943Z",
     "start_time": "2019-10-07T15:41:56.956940Z"
    }
   },
   "outputs": [],
   "source": [
    "def renumber_scId(data):\n",
    "\n",
    "    # Renumber the superCluster\n",
    "    if \"superClusterId\" in data.columns:\n",
    "        firstInSc = data.superClusterId.diff().fillna(1) \n",
    "    else:\n",
    "        firstInSc = data.scId.diff().fillna(1) \n",
    "        \n",
    "    firstInSc[firstInSc != 0] = 1\n",
    "    data['scId'] = firstInSc.cumsum().astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:56.976946Z",
     "start_time": "2019-10-07T15:41:56.969944Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_time_range(data, start_hour = 15, end_hour = 16):\n",
    "    \"\"\"Keep only the rows within a time range\n",
    "    Default start_hour = 15 and end_hour = 16\"\"\"\n",
    "    data_copy_full     = data #.copy()\n",
    "    parsed_timestamps  = data_copy_full.sendingTimeFullStr.apply(lambda x: x.hour)\n",
    "    data_copy_filtered = data_copy_full[(start_hour <= parsed_timestamps) & (parsed_timestamps <= end_hour)]\n",
    "       \n",
    "    # Renumber the superCluster\n",
    "    data_copy_filtered = renumber_scId(data_copy_filtered)\n",
    "\n",
    "    return data_copy_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:56.989950Z",
     "start_time": "2019-10-07T15:41:56.980947Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_short_sc(data, sc_thresh):\n",
    "    \n",
    "    # id like to think this next line should be removable, but not yet.\n",
    "    data = renumber_scId(data)\n",
    "    eventsByScId = pd.DataFrame(data.groupby(['scId']).superClusterEvent.max() > sc_thresh)\n",
    "    filtered = eventsByScId.iloc[data.scId - 1]\n",
    "    \n",
    "    #    filtered['signalId'] = data.index\n",
    "    filtered.insert(1, \"signalId\", data.index, True) \n",
    "    filtered.set_index('signalId')\n",
    "\n",
    "    # Filter out the short clusters\n",
    "    data = data[list(filtered.superClusterEvent)]\n",
    "    \n",
    "    # Renumber the superCluster\n",
    "    data = renumber_scId(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.004954Z",
     "start_time": "2019-10-07T15:41:56.992951Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_no_moves(data, thresh = 0):\n",
    "    \"\"\"remove all superclusters with insignificant target moves\"\"\"\n",
    "    temp = data #.copy()\n",
    "\n",
    "    temp['absTarget'] = temp.target.abs()\n",
    "    nonTrivialScIds = pd.DataFrame(temp.groupby(['scId']).absTarget.max() > thresh)\n",
    "    filtered = nonTrivialScIds.iloc[temp.scId - 1]\n",
    "    filtered.reset_index(level=0, inplace=True)\n",
    "    #    filtered['signalId'] = data.index\n",
    "    filtered.insert(1, \"signalId\", temp.index, True) \n",
    "    filtered.set_index('signalId', inplace=True)\n",
    "\n",
    "    # Filter out the super clusters where the target doesn't move enough\n",
    "    temp = temp[filtered.absTarget]\n",
    "    temp.pop('absTarget')    \n",
    "    \n",
    "    # Renumber the superClusters\n",
    "    temp = renumber_scId(temp)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.013956Z",
     "start_time": "2019-10-07T15:41:57.007954Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_time_since_sc_start(data, time_since_sc_start_thresh):\n",
    "    \"\"\"Remove all rows below a timeSinceSCStart threshold = time_thresh\"\"\"\n",
    "    data = data[data['timeSinceSCStart'] < time_since_sc_start_thresh]\n",
    "    \n",
    "    # Renumber the superClusters\n",
    "    data = renumber_scId(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process a raw DataFrame of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.025960Z",
     "start_time": "2019-10-07T15:41:57.016957Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(data, max_sc_duration=3000, sc_thresh=3):\n",
    "    \"\"\"\n",
    "    This function removes the short superclusters below a certain threshold=sc_thresh,\n",
    "    filters out superclusters with only 0s as targets,\n",
    "    removes all rows with a timeSinceSCStart below a threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    origsize = data.shape[0]\n",
    "    print(\"Processing data...\")\n",
    "    \n",
    "    #FILTER OUT SHORT SC\n",
    "    print(\"Filtering super clusters shorter than \" + str(sc_thresh))\n",
    "    data = filter_short_sc(data, sc_thresh)\n",
    "    \n",
    "    #FILTER OUT SC with NO MOVES in target.\n",
    "    print(\"Filtering non-move causing super clusters\")\n",
    "    data = filter_no_moves(data)\n",
    "\n",
    "    #Filter for time between 3 and 4 pm and remove the long cluster rows.\n",
    "    print(\"Filtering hours\")\n",
    "    data = filter_time_range(data)\n",
    "    \n",
    "    #Filter out all rows beyond a max_sc_duration.\n",
    "    print(\"Filter all events within super cluster after \" + str(max_sc_duration))\n",
    "    data = filter_time_since_sc_start(data, max_sc_duration)\n",
    "    \n",
    "#     print(str(data.shape[0]) + \" left out of \" + str(origsize))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process a range of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.052968Z",
     "start_time": "2019-10-07T15:41:57.028961Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_process_range(fromDate = -1, toDate = -1, list_columns=[], save = False, my_path = \"C:/Barak/SignalToLocal/\",  max_sc_duration=3000, sc_thresh=3, reaction_thresh = 37000000, save_data_path = \"C:/Barak/Python/Processed_Data/\"):\n",
    "    \"\"\"Does the same as load_range but also processes the data according to the default or specified parameters. \"\"\"\n",
    "    default_list = ['scId', 'superClusterEvent', 'timeSinceSCStart', 'sendingTimeFullStr', 'target']\n",
    "    \n",
    "    # TODO make sure this handles column duplicates\n",
    "    my_columns = default_list + list_columns\n",
    "    \n",
    "    concat_data = pd.DataFrame()\n",
    "    first_date = 99999999\n",
    "    last_date = -1\n",
    "    last_scId = 0\n",
    "    for f in listdir(my_path):\n",
    "        # Filter out small files (10 KB)\n",
    "        if os.stat(my_path + f).st_size > 10000:\n",
    "            date = int(f.replace(\"sigsToLocs.\", \"\").replace(\".csv\", \"\"))\n",
    "            if ((fromDate == -1 & toDate == -1) or (fromDate <= date & date <= toDate)):\n",
    "                new_data = load_data_regression(my_path, f, reaction_thresh)\n",
    "                \n",
    "                # Add the last scId value to all the scId columns in the newly loaded file to keep monotonicity\n",
    "                new_data['scId'] += last_scId\n",
    "                \n",
    "\n",
    "                last_scId = new_data.iloc[-1, :]['scId']               \n",
    "                \n",
    "                \n",
    "                new_data = process_data(new_data, max_sc_duration, sc_thresh)\n",
    "                \n",
    "                # Select the requested columns from the loaded file only\n",
    "                if list_columns == []:\n",
    "                    concat_data = pd.concat([concat_data, new_data])\n",
    "                else:\n",
    "                    concat_data = pd.concat([concat_data, new_data[my_columns]])\n",
    "                    \n",
    "                first_date = min(first_date, date)\n",
    "                last_date  = max(last_date , date)\n",
    "\n",
    "                #    if \n",
    "#     concat_data = process_data(concat_data, max_sc_duration, sc_thresh)\n",
    "    # save all loaded concatenated data\n",
    "    if save:\n",
    "        save_file_name = \"processed_\" + str(first_date) + \"_\" + str(last_date) + \".csv\"\n",
    "        print(\"saving \" + save_file_name+ \" to \" + save_data_path)\n",
    "            \n",
    "        if not os.path.exists(save_data_path):\n",
    "            os.makedirs(save_data_path)\n",
    "\n",
    "        concat_data.to_csv(save_data_path + save_file_name)\n",
    "        print(\"Done saving \" + save_file_name)\n",
    "        \n",
    "    return concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.067972Z",
     "start_time": "2019-10-07T15:41:57.055969Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_outliers(X_train, y_train, y_thresh = 2, sigMSP_thresh = 6, sigMND_thresh = 20, sigMDJ_thresh = 10):\n",
    "    \"Remove all the outliers by default value\"\n",
    "    \n",
    "    volMSP_thresh = 999\n",
    "    volMDJ_thresh = 120\n",
    "    volMND_thresh = 400\n",
    "    \n",
    "    in_bounds = ((y_train.abs() <= y_thresh) & \\\n",
    "                 (X_train.signalMSP.abs() <= sigMSP_thresh) & \\\n",
    "                 (X_train.signalMND.abs() <= sigMND_thresh) & \\\n",
    "                 (X_train.signalMDJ.abs() <= sigMDJ_thresh) & \\\n",
    "                 (X_train.signedVolMSP.abs() <= volMSP_thresh) & \\\n",
    "                 (X_train.signedVolMDJ.abs() <= volMDJ_thresh) & \\\n",
    "                 (X_train.signedVolMND.abs() <= volMND_thresh))\n",
    "\n",
    "    X_train_no_outliers = X_train[in_bounds]\n",
    "    y_train_no_outliers = y_train[in_bounds]\n",
    "    \n",
    "    \n",
    "    return X_train_no_outliers, y_train_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the data required for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.095979Z",
     "start_time": "2019-10-07T15:41:57.070973Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_modeling_data(my_path_stored_data = \"C:/Barak/Python/Processed_Data/\", fromDate = -1, toDate = -1, list_columns=[], save = False, my_path_fetch_data = \"C:/Barak/SignalToLocal/\",  max_sc_duration=3000, sc_thresh=3, reaction_thresh = 37000000):\n",
    "\n",
    "    # Load all files in the processed_Data directory\n",
    "    \n",
    "    if not os.path.exists(my_path_stored_data):\n",
    "        os.makedirs(my_path_stored_data)\n",
    "    \n",
    "    files_in_stored_directory = get_file_list(my_path_stored_data) \n",
    "    # Load all files in the signalToLocal directory\n",
    "    \n",
    "    files_in_fetch_directory = get_file_list(my_path_fetch_data)\n",
    "    # Get file string of desired file\n",
    "    processed_filename = \"processed_\" + str(fromDate) + \"_\" + str(toDate) +'.csv'\n",
    "    \n",
    "    # Condition if all files are requested\n",
    "    if fromDate == -1 and toDate == -1:\n",
    "        firstfile = files_in_fetch_directory[0] \n",
    "        lastfile = files_in_fetch_directory[-1]\n",
    "        \n",
    "        first_date = firstfile[-12:-4]\n",
    "        last_date = lastfile[-12:-4]\n",
    "        \n",
    "        print(\"Processing files between \" + str(first_date) + \" and \" + str(last_date))\n",
    "        processed_filename = \"processed_\" + str(first_date) + \"_\" + str(last_date) + '.csv'\n",
    "        \n",
    "    if ((save == False) & (processed_filename in files_in_stored_directory)):\n",
    "        print(\"Loading saved data \" + processed_filename + \" from directory \" + my_path_stored_data + \" ...\")\n",
    "        import_data = pd.read_csv(my_path_stored_data + processed_filename)\n",
    "    else:\n",
    "        print(\"Processing data between \" + str(fromDate) + \" and \" + str(toDate) + \" from directory \" + my_path_fetch_data + \" from scratch...\")\n",
    "        import_data = load_process_range(fromDate = fromDate, toDate = toDate, list_columns = list_columns, save = False, my_path = my_path_fetch_data, max_sc_duration = max_sc_duration, sc_thresh=sc_thresh, reaction_thresh = reaction_thresh)\n",
    "        \n",
    "        if save:\n",
    "            processed_filename = \"processed_\" + str(fromDate) + \"_\" + str(toDate) + \".csv\"\n",
    "            print(\"Saving \" + processed_filename + \" to directory \" + my_path_stored_data + \" ...\")\n",
    "            if not os.path.exists(my_path_stored_data):\n",
    "                os.makedirs(my_path_stored_data)\n",
    "\n",
    "            import_data.to_csv(my_path_stored_data + processed_filename)\n",
    "    print(\"Done loading files!\")\n",
    "    \n",
    "    return import_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.111984Z",
     "start_time": "2019-10-07T15:41:57.098980Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_split_regression(df, test_ratio):\n",
    "    \"\"\"Splits the data by time series split based on a test_ratio proportion\n",
    "    test_ratio = valid_ratio\n",
    "    train_ratio = 1 - test_ratio - valid_ratio\"\"\"\n",
    "    \n",
    "    train_size = int(df.shape[0] * (1 - 2 * test_ratio))\n",
    "    valid_size = int(df.shape[0] * test_ratio)\n",
    "    \n",
    "    X_train = df.iloc[:train_size,:].drop(columns=\"target\")\n",
    "    X_valid = df.iloc[train_size:train_size + valid_size, :].drop(columns=\"target\")\n",
    "    X_test = df.iloc[train_size + valid_size:, :].drop(columns=\"target\")\n",
    "    y_train = df.iloc[:train_size,:]['target']\n",
    "    y_valid = df.iloc[train_size:train_size + valid_size,:]['target']\n",
    "    y_test = df.iloc[train_size + valid_size:,:]['target']\n",
    "    return X_train, X_test, X_valid, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.128989Z",
     "start_time": "2019-10-07T15:41:57.114985Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_sc(data, toPlot):\n",
    "    toShow = data[data.scId == toPlot]\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.plot(toShow.timeSinceSCStart , toShow.signalMSP, color='red', marker='.')\n",
    "    plt.plot(toShow.timeSinceSCStart , toShow.signalMDJ, color='green', marker='.')\n",
    "    plt.plot(toShow.timeSinceSCStart , toShow.signalMND, color='blue', marker='.')\n",
    "    plt.title(str(toPlot) + ' @ ' + toShow.sendingTimeFullStr.iloc[0])\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(toShow.timeSinceSCStart , toShow.signedVolMSP, color='red', marker='.')\n",
    "    plt.plot(toShow.timeSinceSCStart , toShow.signedVolMDJ, color='green', marker='.')\n",
    "    plt.plot(toShow.timeSinceSCStart , toShow.signedVolMND, color='blue', marker='.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.143993Z",
     "start_time": "2019-10-07T15:41:57.131990Z"
    }
   },
   "outputs": [],
   "source": [
    "def cap_by_quantile(toCap, quant):\n",
    "    thresh = 0.1\n",
    "    print('filtered ' + str((abs(toCap) <= thresh).sum()) + ' out of ' + str(toCap.count()))\n",
    "    toCap = toCap[abs(toCap) > thresh]\n",
    "    cap = toCap.abs().quantile(quant)\n",
    "    capped = sum(abs(toCap) > cap)\n",
    "    toCap[abs(toCap) > cap] = cap\n",
    "    sns.distplot(toCap)\n",
    "    plt.title(str(quant) + '% -> ' + str(cap))\n",
    "    print('capped ' + str(capped) + ' out of ' + str(toCap.count()))\n",
    "    print(toCap.describe().apply(lambda x: format(x, 'f')))\n",
    "    return toCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.150995Z",
     "start_time": "2019-10-07T15:41:57.146994Z"
    }
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import sklearn\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# def dbscan_outlier_plot(X_train, y_train, field):\n",
    "#     \"\"\"data: pandas dataframe\n",
    "#        field: field in dataframe with outliers to be removed.\"\"\"\n",
    "#     dbs_model = DBSCAN(eps=0.05, min_samples=50).fit(X_train[field])\n",
    "#     colors = dbs_model.labels_\n",
    "#     f = plt.figure()\n",
    "#     plt.scatter(X_train[field], y_train, c = dbs_model, s=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.160998Z",
     "start_time": "2019-10-07T15:41:57.153996Z"
    }
   },
   "outputs": [],
   "source": [
    "def cor_nolabels(train):\n",
    "    \"Plot the correlation matrix of all the features and without the correlation values on them\"\n",
    "    f, ax = plt.subplots(figsize=(10, 8))\n",
    "    corr = train.corr()\n",
    "    sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "                square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.170001Z",
     "start_time": "2019-10-07T15:41:57.163999Z"
    }
   },
   "outputs": [],
   "source": [
    "def cor_labels(train):\n",
    "    \"Plot the correlation matrix of all the features and the correlation values on them\"\n",
    "    f = plt.figure(figsize=(15,7))\n",
    "    ax = sns.heatmap(train.corr(), annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.184005Z",
     "start_time": "2019-10-07T15:41:57.173001Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_labeled(X, Y, predict_labels, my_labels, colors, x_axis, y_axis, plot_title):\n",
    "    \"\"\"Plots a scatter of 2 features and their colors depending on their labels.\n",
    "    X = Data in X Axis\n",
    "    Y = Data in Y Axis\n",
    "    predict_labels = the list of every points label\n",
    "    colors=[\"red\", \"blue\"]\n",
    "    my_labels=[0,1]\n",
    "    x_axis = Title of x axis\n",
    "    y_axis = Title of y axis\n",
    "    plot_title -= Title of the plot.\"\"\"\n",
    "    f = plt.figure()\n",
    "\n",
    "    plt.scatter(X, Y, c=predict_labels, cmap=ListedColormap(colors));\n",
    "    plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    plt.title(plot_title)\n",
    "    cb = plt.colorbar()\n",
    "    loc = np.arange(0,max(predict_labels),max(predict_labels)/float(len(colors)))\n",
    "    cb.set_ticks(loc)\n",
    "    cb.set_ticklabels(my_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distributions of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.202010Z",
     "start_time": "2019-10-07T15:41:57.191007Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_continuous_distributions(X_train, continuous_features):\n",
    "    \"\"\"Plots all the distributions in continuous_features which is a subset of the features in X_train\"\"\"\n",
    "\n",
    "    for my_feature in tqdm(continuous_features):\n",
    "        f = plt.figure(figsize=(10,5))\n",
    "        sns.distplot(X_train[my_feature], kde=True, hist=False, \n",
    "        color = 'darkblue', \n",
    "        kde_kws={'linewidth': 4}).set_title(\"Density plot of {}\".format(my_feature)); \n",
    "        plt.ylabel('Density')\n",
    "        plt.xlabel(my_feature,fontsize = 15)\n",
    "    \n",
    "        # Show the grid lines as dark grey lines\n",
    "        plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.218014Z",
     "start_time": "2019-10-07T15:41:57.206011Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_categorical_distributions(X_train, categoric_features): \n",
    "    \"\"\"Plots all the histograms in categoric_features which is a subset of the features in X_train\"\"\"\n",
    "    for my_feature in tqdm(categoric_features):\n",
    "        f = plt.figure(figsize=(10,5))\n",
    "        plt.style.use('ggplot')\n",
    "        my_dist= X_train[my_feature].value_counts()\n",
    "        x = my_dist.index\n",
    "        my_counts = my_dist.values\n",
    "        x_pos = [i for i, _ in enumerate(x)]\n",
    "        plt.bar(x_pos, my_counts, color='blue')\n",
    "        plt.xlabel(my_feature,fontsize = 15)\n",
    "        plt.ylabel(\"{} Frequency\".format(my_feature), fontsize = 15)\n",
    "        plt.title(\"Distribution of {}\".format(my_feature),fontsize = 18)\n",
    "        plt.xticks(x_pos, x, rotation='vertical')\n",
    "        \n",
    "        # Show the grid lines as dark grey lines\n",
    "        plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of relationship between features and dependnt variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.229018Z",
     "start_time": "2019-10-07T15:41:57.221015Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_continuous_vs_target(X_train, y_train, continuous_features):\n",
    "    \"\"\"Plots all the features in continuous_features vs the target\"\"\"\n",
    "    for my_feature in tqdm(continuous_features):\n",
    "        f = plt.figure()\n",
    "        plt.scatter(X_train[my_feature], y_train, color='blue');\n",
    "        plt.ylabel('Target');\n",
    "        plt.xlabel(my_feature);\n",
    "\n",
    "        # Show the grid lines as dark grey lines\n",
    "        plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "        plt.title(\"Target vs. {}\".format(my_feature),fontsize = 18)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot with boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.235019Z",
     "start_time": "2019-10-07T15:41:57.232019Z"
    }
   },
   "outputs": [],
   "source": [
    "# ax = plt.subplot(111)\n",
    "# ax.plot(np.arange(0,1,0.005)[0:100], train_freq[0:100], label = \"train\")\n",
    "# ax.plot(np.arange(0,1,0.005)[0:100], valid_freq[0:100], label = \"valid\")\n",
    "# ax.figure.set_size_inches(10, 10)\n",
    "# plt.xlabel('Cutoff')\n",
    "# plt.ylabel('Trade Freq')\n",
    "# plt.title('Plot of Trade Freq vs cutoff')\n",
    "# plt.axis('tight')\n",
    "# plt.legend(bbox_to_anchor=(1, 1), ncol=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.255025Z",
     "start_time": "2019-10-07T15:41:57.238020Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_modeling_process(my_model, test_thresh, predict_thresh, my_path_stored_data = \"C:/Barak/Python/Processed_Data/\", fromDate = -1, toDate = -1, list_columns=[], save = False, my_path_fetch_data = \"C:/Barak/SignalToLocal/\",  max_sc_duration=3000, sc_thresh=3, reaction_thresh = 37000000, test_size = 0.2, my_labels = [-1,0,1]):\n",
    "    \"\"\"Runs an entire modeling process with no need for pre processing.\n",
    "    -Imports and processes the data in the dat range.\n",
    "    -Splits the data\n",
    "    -Removes the outliers.\n",
    "    -Fits the model\n",
    "    -Predicts and evaluates on the test set\n",
    "    Returns the model, the train and test data and its predicted and test labels\"\"\"\n",
    "    #Import data\n",
    "    import_data = get_modeling_data(my_path_stored_data = my_path_stored_data, fromDate = fromDate, toDate = toDate, list_columns=list_columns, save = save, my_path_fetch_data = my_path_fetch_data,  max_sc_duration= max_sc_duration, sc_thresh=sc_thresh, reaction_thresh = reaction_thresh)\n",
    "    \n",
    "    #Split data\n",
    "    print(\"Splitting data...\")\n",
    "    #fix for split function\n",
    "    test_size=test_size/2\n",
    "    X_train, X_test, X_valid, y_train, y_valid, y_test = data_split_regression(import_data, test_size)\n",
    "    X_test = pd.concat([X_test, X_valid])\n",
    "    y_test = pd.concat([y_test, y_valid])\n",
    "    #Remove outliers\n",
    "    print(\"Removing Outliers...\")\n",
    "    X_train_no_outliers, y_train_no_outliers = remove_outliers(X_train, y_train)\n",
    "    \n",
    "    #Fit model\n",
    "    print(\"Fitting Model...\")\n",
    "    my_model.fit(X_train_no_outliers[list_columns], y_train_no_outliers)\n",
    "    \n",
    "    print(\"Predicting on test...\")\n",
    "    y_pred = my_model.predict(X_test[list_columns])\n",
    "    \n",
    "    y_test_labels = make_regression_labels(test_thresh, y_test)\n",
    "    y_pred_labels = make_regression_labels(predict_thresh, y_pred)\n",
    "    print(\"----------------------\")\n",
    "    print_precisions(y_test_labels, y_pred_labels)\n",
    "    \n",
    "    print_conf_matrix(y_test_labels, y_pred_labels, my_labels)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return my_model, X_train_no_outliers, y_train_no_outliers, X_test, y_test, y_pred_labels, y_test_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.406068Z",
     "start_time": "2019-10-07T15:41:57.258026Z"
    }
   },
   "outputs": [],
   "source": [
    "#def get_modeling_data(my_path_stored_data = \"C:/Barak/Python/Processed_Data/\", fromDate = -1, toDate = -1, list_columns=[], save = False, my_path_fetch_data = \"C:/Barak/SignalToLocal/\",  max_sc_duration=3000, sc_thresh=3, reaction_thresh = 37000000):\n",
    "from nyoka import skl_to_pmml\n",
    "from nyoka import xgboost_to_pmml\n",
    "from sklearn2pmml import sklearn2pmml\n",
    "from sklearn2pmml.pipeline import PMMLPipeline\n",
    "from sklearn2pmml import make_pmml_pipeline\n",
    "\n",
    "def pmml_export(my_model, X_train, filename, pmml_path = \"C:/Barak/Python/PMML/\", downgrade = True, save_pickle = True, use_nyoka = False):\n",
    "    \"\"\"Model must be trained and in an sklearn Pipeline.\n",
    "       X_train and y_train must be in pandas format.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(pmml_path):\n",
    "        os.makedirs(pmml_path)\n",
    "    \n",
    "    full_filename = pmml_path + filename + \".pmml\"\n",
    "\n",
    "    target = \"prediction\"\n",
    "    print(\"Saving PMML to \" + full_filename)\n",
    "    # check https://pypi.org/project/nyoka/\n",
    "    if use_nyoka == True:\n",
    "        if \"XGB\" in filename:\n",
    "            xgboost_to_pmml(my_model, X_train.columns.values, target, full_filename)\n",
    "        else:\n",
    "            skl_to_pmml(my_model, X_train.columns.values, target, full_filename)\n",
    "        \n",
    "        if downgrade:\n",
    "            print(\"Downgrading PMML to 4.3\")\n",
    "            with open(full_filename, \"rt\") as fin:\n",
    "                with open(full_filename, \"wt\") as fout:\n",
    "                    for line in fin:\n",
    "                        fout.write(line.replace('\"4.4\"', '\"4.3\"').replace('-4_4', '-4_3'))\n",
    "\n",
    "            # remove non-downgraded\n",
    "            os.remove(full_filename)\n",
    "    else:\n",
    "        pipeline = make_pmml_pipeline(my_model, X_train.columns.values, target)\n",
    "        sklearn2pmml(pipeline, full_filename)\n",
    "        \n",
    "    if save_pickle:\n",
    "        save_model(my_model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.416072Z",
     "start_time": "2019-10-07T15:41:57.409069Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(my_model, filename, filepath = \"C:/Barak/Python/FinalizedModels/\"):\n",
    "    \"Saves my_model to a pickle file in the woring directory with the name filename.\"\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "        \n",
    "    print(\"Saving \" + filename + \" to \" + filepath)\n",
    "    pickle.dump(my_model, open(filepath + filename + \".sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from sav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.426074Z",
     "start_time": "2019-10-07T15:41:57.419072Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(filename, filepath = \"C:/Barak/Python/FinalizedModels/\"):\n",
    "    \"Loads a pickled model in filename\"\n",
    "  \n",
    "    print(\"Loading \" + filename + \" from \" + filepath)\n",
    "    loaded_model = pickle.load(open(filepath + filename, 'rb'))\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.440078Z",
     "start_time": "2019-10-07T15:41:57.429076Z"
    }
   },
   "outputs": [],
   "source": [
    "def bar_plot(my_labels, x_axis='Profitability'):\n",
    "    df = pd.DataFrame(my_labels, columns=[x_axis])\n",
    "    ax = sns.countplot(y=x_axis, data=df)\n",
    "    plt.title('Distribution of {}'.format(x_axis))\n",
    "    plt.xlabel('Number of {} Transactions'.format(x_axis))\n",
    "\n",
    "    total = len(df[x_axis])\n",
    "    for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n",
    "        x = p.get_x() + p.get_width() + 0.02\n",
    "        y = p.get_y() + p.get_height() / 2\n",
    "        ax.annotate(percentage, (x, y))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:36:02.570315Z",
     "start_time": "2019-09-19T09:36:02.567314Z"
    }
   },
   "source": [
    "#### Precision Function - FP : Predict 1 or -1 and get 0 OR predict 1 and get -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.449080Z",
     "start_time": "2019-10-07T15:41:57.443079Z"
    }
   },
   "outputs": [],
   "source": [
    "def normal_precision(y_true, y_pred):\n",
    "    \"\"\" Precision Function - FP : Predict 1 or -1 and get 0 OR predict 1 and get -1 OR predict -1 and get 1\"\"\"\n",
    "    predictions  = (y_pred != 0).sum()\n",
    "    correct_pred = (y_true * y_pred == 1).sum()\n",
    "    return 0 if predictions == 0 else correct_pred / predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Function - FP : Predict 1 or -1 and get opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.458083Z",
     "start_time": "2019-10-07T15:41:57.452081Z"
    }
   },
   "outputs": [],
   "source": [
    "def penalty_precision(y_true, y_pred):\n",
    "    \"\"\"Precision Function - FP : Predict 1 or -1 and get opposite\"\"\"\n",
    "    predictions = (y_pred != 0).sum()\n",
    "    wrong_pred  = (y_true * y_pred == -1).sum()\n",
    "    return 0 if predictions == 0 else wrong_pred / predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Function - FP : Predict 1 or -1 and get 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.467085Z",
     "start_time": "2019-10-07T15:41:57.461084Z"
    }
   },
   "outputs": [],
   "source": [
    "def indiff_precision(y_true, y_pred):\n",
    "    \"\"\"Precision Function - FP : Predict 1 or -1 and get 0\"\"\"\n",
    "    predictions  = (y_pred != 0).sum()\n",
    "    false_pred   = ((y_pred != 0) & (y_true == 0)).sum()\n",
    "    return 0 if predictions == 0 else false_pred / predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.476089Z",
     "start_time": "2019-10-07T15:41:57.470086Z"
    }
   },
   "outputs": [],
   "source": [
    "def utilization_precision(y_true, y_pred):\n",
    "    \"\"\"Precision Function - FP : Predict 1 or -1 and get 0\"\"\"\n",
    "    predictions   = (y_pred != 0).sum()\n",
    "    opportunities = (y_true != 0).sum()\n",
    "    return 0 if opportunities == 0 else predictions / opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print all the precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.487091Z",
     "start_time": "2019-10-07T15:41:57.479089Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_precisions(y_test, y_pred):\n",
    "    \"\"\"Print all 3 precisions where y_test = y_true and y_pred is the model predictions\"\"\"\n",
    "    nor_prec = normal_precision(y_test, y_pred)\n",
    "    pen_prec = penalty_precision(y_test, y_pred)\n",
    "    ind_prec = indiff_precision(y_test, y_pred)\n",
    "    utl_prec = utilization_precision(y_test, y_pred)\n",
    "    print(\"Utilization:           {:,.3f}\".format(utl_prec))\n",
    "    \n",
    "    print(\"Correct Precision:     {:,.3f}\".format(nor_prec))\n",
    "    print(\"Mistaken Precision:    {:,.3f}\".format(pen_prec))\n",
    "    print(\"Indifferent Precision: {:,.3f}\".format(ind_prec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T09:45:43.526543Z",
     "start_time": "2019-09-19T09:45:43.523543Z"
    }
   },
   "source": [
    "### Plot the precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.567114Z",
     "start_time": "2019-10-07T15:41:57.490091Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_precisions(max_depth_list, n_est_list, train_penalty_dict, valid_penalty_dict):\n",
    "    ax1 = plt.subplot(111)\n",
    "    for n in n_est_list:\n",
    "        ax1.plot(max_depth_list[1:], train_penalty_dict[n][1:], label = \"n_est: {}\".format(n))\n",
    "    ax1.figure.set_size_inches(10, 10)\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Penalty Precision')\n",
    "    plt.title('TRAIN: Plot of Penalty Precision vs Max depth for several Number of estimators for a Random Tree model')\n",
    "    plt.axis('tight')\n",
    "    plt.legend(bbox_to_anchor=(1, 1), ncol=2)\n",
    "    plt.show()\n",
    "    \n",
    "    ax2 = plt.subplot(111)\n",
    "    for n in n_est_list:\n",
    "        ax2.plot(max_depth_list[1:], valid_penalty_dict[n][1:], label = \"n_est: {}\".format(n))\n",
    "    ax2.figure.set_size_inches(10, 10)\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Penalty Precision')\n",
    "    plt.title('VALIDATION: Plot of Penalty Precision vs Max depth for several Number of estimators for a Random Tree model')\n",
    "    plt.axis('tight')\n",
    "    plt.legend(bbox_to_anchor=(1, 1), ncol=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.592121Z",
     "start_time": "2019-10-07T15:41:57.570114Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomforest_tuning(X_train, y_train, X_valid, y_valid, n_est_list, max_depth_list, list_columns, true_thresh=0, predict_thresh=0.2):\n",
    "    \"\"\"Tunes the hyperparameters of a random forest regressor.\"\"\"\n",
    "    train_penalty_dict = {}\n",
    "    valid_penalty_dict = {}\n",
    "\n",
    "    train_normal_dict = {}\n",
    "    valid_normal_dict = {}\n",
    "\n",
    "    train_indiff_dict = {}\n",
    "    valid_indiff_dict = {}\n",
    "\n",
    "    for n in tqdm(n_est_list):\n",
    "        train_penalty_dict[n] = []\n",
    "        valid_penalty_dict[n] = []\n",
    "\n",
    "        train_normal_dict[n] = []\n",
    "        valid_normal_dict[n] = []\n",
    "\n",
    "        train_indiff_dict[n] = []\n",
    "        valid_indiff_dict[n] = []\n",
    "\n",
    "        for my_depth in max_depth_list:\n",
    "\n",
    "            clf = RandomForestRegressor(n_estimators=n, criterion=\"mse\", max_depth=my_depth)\n",
    "            clf.fit(X_train[list_columns], y_train)\n",
    "\n",
    "            y_pred_train = clf.predict(X_train[list_columns])\n",
    "            y_pred_valid = clf.predict(X_valid[list_columns])\n",
    "\n",
    "            y_train_labels =  make_regression_labels(true_thresh, y_train)\n",
    "            y_pred_train_labels = make_regression_labels(predict_thresh, y_pred_train)\n",
    "\n",
    "            y_valid_labels =  make_regression_labels(true_thresh, y_valid)\n",
    "            y_pred_valid_labels = make_regression_labels(predict_thresh, y_pred_valid)\n",
    "\n",
    "            train_penalty_dict[n].append(penalty_precision(y_train_labels, y_pred_train_labels))\n",
    "            valid_penalty_dict[n].append(penalty_precision(y_valid_labels, y_pred_valid_labels))\n",
    "\n",
    "            train_normal_dict[n].append(normal_precision(y_train_labels, y_pred_train_labels))\n",
    "            valid_normal_dict[n].append(normal_precision(y_valid_labels, y_pred_valid_labels))\n",
    "\n",
    "            train_indiff_dict[n].append(indiff_precision(y_train_labels, y_pred_train_labels))\n",
    "            valid_indiff_dict[n].append(indiff_precision(y_valid_labels, y_pred_valid_labels))\n",
    "\n",
    "        print(\"train: n_est={}, score:{}\".format(n, train_penalty_dict[n]))\n",
    "        print(\"valid: n_est={}, score:{}\".format(n, valid_penalty_dict[n]))\n",
    "        print(\"----------------------------------------\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"----------------------------------------\")\n",
    "    return train_penalty_dict, valid_penalty_dict, train_normal_dict, valid_normal_dict, train_indiff_dict, valid_indiff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.610127Z",
     "start_time": "2019-10-07T15:41:57.595122Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_training_days(my_model, X_train, y_train, X_test, y_test, my_features, n_step=1):\n",
    "    \"Make sure to include the sendingTimeFullStr in the Training set\"\n",
    "    pen_prec = []\n",
    "    norm_prec=[]\n",
    "    indiff_prec = []\n",
    "    \n",
    "    start_date = X_train.loc[:, 'sendingTimeFullStr'][0]\n",
    "    end_date = X_train.loc[:, 'sendingTimeFullStr'][-1]\n",
    "    \n",
    "    delta = start_date - end_date\n",
    "    \n",
    "    total_days = delta.days\n",
    "\n",
    "    for n_days in tqdm(range(1, total_days, n_step)):\n",
    "        \n",
    "        date_cutoff = end_date - datetime.timedelta(days=n_days)\n",
    "        \n",
    "        date_filter = X_train.sendingTimeFullStr>=date_cutoff\n",
    "        X_train_temp = X_train.iloc[date_filter][my_features]\n",
    "        y_train_temp = y_train.iloc[date_filter]\n",
    "\n",
    "        my_model.fit(X_train_temp, y_train_temp)\n",
    "        y_pred = my_model.predict(X_test[my_features])\n",
    "        y_test_labels =  make_regression_labels(0,y_test)\n",
    "        y_pred_labels = make_regression_labels(0,y_pred)\n",
    "\n",
    "        pen_prec.append(penalty_precision(y_test_labels,y_pred_labels))\n",
    "        norm_prec.append(normal_precision(y_test_labels,y_pred_labels))\n",
    "        indiff_prec.append(indiff_precision(y_test_labels,y_pred_labels))\n",
    "        \n",
    "    return pen_prec, norm_prec, indiff_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.620130Z",
     "start_time": "2019-10-07T15:41:57.612128Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_conf_matrix(y_test, y_pred, my_labels):\n",
    "    ax = plt.subplot()\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot = True, fmt = \"d\")\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.xaxis.set_ticklabels(my_labels)\n",
    "    ax.yaxis.set_ticklabels(my_labels)\n",
    "    \n",
    "    ax.set_title('Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.635133Z",
     "start_time": "2019-10-07T15:41:57.623130Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_labeled_many(X, Y, predict_labels, continuous_features, my_labels, colors):\n",
    "    \"\"\"Plots a scatter of 2 features and their colors depending on their labels.\n",
    "    X = Data in X Axis\n",
    "    Y = Data in Y Axis\n",
    "    predict_labels = the list of every points label\n",
    "    colors=[\"red\", \"blue\"]\n",
    "    my_labels=[0,1]\n",
    "    x_axis = Title of x axis\n",
    "    y_axis = Title of y axis\n",
    "    plot_title -= Title of the plot.\"\"\"\n",
    "    \n",
    "    for my_feature in tqdm(continuous_features):\n",
    "        f = plt.figure()\n",
    "        plt.scatter(X[my_feature], Y, c=predict_labels, s=10, cmap=ListedColormap(colors));\n",
    "        plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "        plt.xlabel(my_feature)\n",
    "        plt.ylabel(\"True target\")\n",
    "        plt.title(\"Target vs {}\".format(my_feature))\n",
    "        cb = plt.colorbar()\n",
    "        loc = np.arange(0, max(predict_labels), max(predict_labels) / float(len(colors)))\n",
    "        cb.set_ticks(loc)\n",
    "        cb.set_ticklabels(my_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.641136Z",
     "start_time": "2019-10-07T15:41:57.638135Z"
    }
   },
   "outputs": [],
   "source": [
    "# deriv_df = pd.DataFrame()\n",
    "# deriv_df['time'] = firsts.sendingTime[shft:]\n",
    "# deriv_df['speed'] = speed[shft:]\n",
    "# time_cutoff = deriv_df[deriv_df.speed == deriv_df.speed.max()].time\n",
    "# new_train = train[train.sendingTime>=int(time_cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T15:41:57.648138Z",
     "start_time": "2019-10-07T15:41:57.644137Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pyodbc\n",
    "# def queryFromDB(DB_DATABASE, bunch, date, startTime, endTime):\n",
    "#     DB_USER = 'sa'\n",
    "#     DB_PASS = 'barak123!@#'\n",
    "#     DB_SERVER = '192.168.50.102'\n",
    "\n",
    "#     cnx_params = \"Driver={SQL Server};\" + \"\"\"Server={0};\n",
    "#                       UID={1};\n",
    "#                       PWD={2};\n",
    "#                       Database={3};\"\"\".format(DB_SERVER,\n",
    "#                                               DB_USER,\n",
    "#                                               DB_PASS,\n",
    "#                                               DB_DATABASE)\n",
    "#     cnx = pyodbc.connect(cnx_params, autocommit=True)\n",
    "\n",
    "#     query = \"spGetMarket @startingTime='\" + startTime + \"', @endingTime='\" + endTime + \"', @bunch='\" + bunch + \"', @date='\" + date + \"', @columns='gwRequestTime, sendingTime, sendingTimeFullStr, eventId, sequence, event, simpleMid, mid' \"\n",
    "#     target = pd.read_sql(query, cnx)\n",
    "\n",
    "#     return target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
